df %>%
mutate(rnd = runif(nrow(df))) %>%
filter(rnd < sample_size) %>%
select(-rnd)
rnd = runif(nrow(df))
# partition
df_test <- df[rnd > 0.8, ]
df_train <- df[rnd <= 0.8, ]
rm(df, rnd, sample_size)
# conversion to factors for smote
binary_columns <- c(2, 7, 8, 19, 20, 28, 29, 30, 31, 32, 33,
34, 35, 36, 37, 38, 39, 40, 41, 42, 43,
44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
154, 155, 156, 157, 158, 159, 160, 161,
162, 163, 164, 165, 166, 167, 168, 169,
170, 171)
binary_colnames <- as.character(lapply(binary_columns, function(x) colnames(df_train)[x]))
df_train[binary_colnames] <- lapply(df_train[binary_colnames], as.factor)
task = makeClassifTask(data = df_train, target = "was_promoted")
task.under = undersample(task, rate = 1/4)
task.smote = smote(task.under, rate = 11, nn = 5)
df_train <- getTaskData(task.smote)
df_train[binary_colnames] <- lapply(df_train[binary_colnames], function(x) as.numeric(as.vector(x)))
rm(task, task.under, task.smote, binary_colnames, binary_columns)
# Neural
y_train <-
df_train %>%
select(was_promoted) %>%
as.matrix() %>%
to_categorical()
x_train <-
df_train %>%
select(-was_promoted) %>%
as.matrix()
rm(df_train)
y_test <-
df_test %>%
select(was_promoted) %>%
as.matrix() %>%
to_categorical()
x_test <-
df_test %>%
select(-was_promoted) %>%
as.matrix()
rm(df_test)
rm(model, history)
model <- keras_model_sequential()
model %>%
layer_dense(units = 128, input_shape = c(dim(x_train)[2]))  %>%
layer_activation_leaky_relu(alpha = 0.3) %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 64, activation = 'relu', input_shape = c(dim(x_train)[2]))  %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 2, activation = 'softmax')
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_adam(lr=0.008),
metrics = c('accuracy')
)
history <- model %>% fit(
x_train, y_train,
epochs = 10, batch_size = 128,
validation_split = 0.3,
#class_weight = class_weight
)
rm(model, history)
model <- keras_model_sequential()
model %>%
layer_dense(units = 128, input_shape = c(dim(x_train)[2]))  %>%
layer_activation_leaky_relu(alpha = 0.3) %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 64, activation = 'relu', input_shape = c(dim(x_train)[2]))  %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 2, activation = 'softmax')
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_adam(lr=0.008),
metrics = c('accuracy')
)
history <- model %>% fit(
x_train, y_train,
epochs = 10, batch_size = 128,
validation_split = 0.3
)
##### ANN #####
rm(list=ls())
# load the data + sample
load('C:/Users/Samuel/Documents/ML_Naspers/Data/dataset_kerasready.RData')
sample_size <- 0.1
df <-
df %>%
mutate(rnd = runif(nrow(df))) %>%
filter(rnd < sample_size) %>%
select(-rnd)
rnd = runif(nrow(df))
# partition
df_test <- df[rnd > 0.8, ]
df_train <- df[rnd <= 0.8, ]
rm(df, rnd, sample_size)
# Neural
y_train <-
df_train %>%
select(was_promoted) %>%
as.matrix() %>%
to_categorical()
x_train <-
df_train %>%
select(-was_promoted) %>%
as.matrix()
rm(df_train)
y_test <-
df_test %>%
select(was_promoted) %>%
as.matrix() %>%
to_categorical()
x_test <-
df_test %>%
select(-was_promoted) %>%
as.matrix()
rm(df_test)
rm(model, history)
model <- keras_model_sequential()
model %>%
layer_dense(units = 128, input_shape = c(dim(x_train)[2]))  %>%
layer_activation_leaky_relu(alpha = 0.3) %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 64, activation = 'relu', input_shape = c(dim(x_train)[2]))  %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 2, activation = 'softmax')
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_adam(lr=0.008),
metrics = c('accuracy')
)
history <- model %>% fit(
x_train, y_train,
epochs = 10, batch_size = 128,
validation_split = 0.3
)
rm(model, history)
model <- keras_model_sequential()
model %>%
layer_dense(units = 128, input_shape = c(dim(x_train)[2]))  %>%
layer_activation_leaky_relu(alpha = 0.3) %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 64, activation = 'relu', input_shape = c(dim(x_train)[2]))  %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 2, activation = 'softmax')
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_adam(lr=0.008),
metrics = c('accuracy')
)
history <- model %>% fit(
x_train, y_train,
epochs = 10, batch_size = 128,
validation_split = 0.3
)
class(x_train)
str(x_train)
View(head(x_train))
View(head(as.numeric(x_train)))
x_train[1, 1]
x_train[1:2, 1]
x_train[1:33, 1]
##### ANN #####
rm(list=ls())
# load the data + sample
load('C:/Users/Samuel/Documents/ML_Naspers/Data/dataset_kerasready.RData')
sample_size <- 0.1
df <-
df %>%
mutate(rnd = runif(nrow(df))) %>%
filter(rnd < sample_size) %>%
select(-rnd)
rnd = runif(nrow(df))
# partition
df_test <- df[rnd > 0.8, ]
df_train <- df[rnd <= 0.8, ]
rm(df, rnd, sample_size)
# conversion to factors for smote
binary_columns <- c(2, 7, 8, 19, 20, 28, 29, 30, 31, 32, 33,
34, 35, 36, 37, 38, 39, 40, 41, 42, 43,
44, 45, 46, 47, 48, 49, 50, 51, 52, 53,
154, 155, 156, 157, 158, 159, 160, 161,
162, 163, 164, 165, 166, 167, 168, 169,
170, 171)
binary_colnames <- as.character(lapply(binary_columns, function(x) colnames(df_train)[x]))
df_train[binary_colnames] <- lapply(df_train[binary_colnames], as.factor)
task = makeClassifTask(data = df_train, target = "was_promoted")
task.under = undersample(task, rate = 1/4)
task.smote = smote(task.under, rate = 11, nn = 5)
df_train <- getTaskData(task.smote)
all_cols <- colnames(df_train)
df_train[all_cols] <- lapply(df_train[all_cols], function(x) as.numeric(as.vector(x)))
str(df_train)
df_train <- getTaskData(task.smote)
numeric_cols <- setdiff(colnames(df_train), binary_colnames)
df_train[numeric_cols] <- lapply(df_train[numeric_cols], function(x) as.integer(as.vector(x)))
df_train[binary_colnames] <- lapply(df_train[binary_colnames], function(x) as.numeric(as.vector(x)))
str(df_train)
df_train <- getTaskData(task.smote)
str(df_train)
df_train[binary_colnames] <- lapply(df_train[binary_colnames], function(x) as.numeric(as.vector(x)))
str(df_train)
rm(task, task.under, task.smote, binary_colnames, binary_columns)
# Neural
y_train <-
df_train %>%
select(was_promoted) %>%
unname() %>%
as.matrix() %>%
to_categorical()
x_train <-
df_train %>%
select(-was_promoted) %>%
unname() %>%
as.matrix()
rm(df_train)
y_test <-
df_test %>%
select(was_promoted) %>%
unname() %>%
as.matrix() %>%
to_categorical()
x_test <-
df_test %>%
select(-was_promoted) %>%
unname() %>%
as.matrix()
rm(df_test)
rm(model, history)
model <- keras_model_sequential()
model %>%
layer_dense(units = 128, input_shape = c(dim(x_train)[2]))  %>%
layer_activation_leaky_relu(alpha = 0.3) %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 64, activation = 'relu', input_shape = c(dim(x_train)[2]))  %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 2, activation = 'softmax')
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_adam(lr=0.008),
metrics = c('accuracy')
)
history <- model %>% fit(
x_train, y_train,
epochs = 10, batch_size = 128,
validation_split = 0.3
)
class(x_train[,1])
class(x_train[1,1])
##### ANN #####
rm(list=ls())
# load the data + sample
load('C:/Users/Samuel/Documents/ML_Naspers/Data/dataset_kerasready.RData')
str(df)
sample_size <- 0.1
df <-
df %>%
mutate(rnd = runif(nrow(df))) %>%
filter(rnd < sample_size) %>%
select(-rnd)
str(df)
rnd = runif(nrow(df))
# partition
df_test <- df[rnd > 0.8, ]
df_train <- df[rnd <= 0.8, ]
str(df_train)
rm(df, rnd, sample_size)
# Neural
y_train <-
df_train %>%
select(was_promoted) %>%
unname() %>%
as.matrix() %>%
to_categorical()
x_train <-
df_train %>%
select(-was_promoted) %>%
unname() %>%
as.matrix()
X <- as.matrix(df_train)
X <- as.numeric(X)
X[[1]]
class(X[[1]])
View(str(df_train))
X <- str(df_train)
load('C:/Users/Samuel/Documents/ML_Naspers/Data/dataset_kerasready.RData')
class(df)
sample_size <- 0.1
df <-
df %>%
mutate(rnd = runif(nrow(df))) %>%
filter(rnd < sample_size) %>%
select(-rnd)
rnd = runif(nrow(df))
df_test <- df[rnd > 0.8, ]
df_train <- df[rnd <= 0.8, ]
rm(df)
y_train <-
df_train %>%
select(was_promoted) %>%
as.matrix() %>%
to_categorical()
x_train <-
df_train %>%
select(-was_promoted) %>%
as.matrix()
rm(df_train)
y_test <-
df_test %>%
select(was_promoted) %>%
as.matrix() %>%
to_categorical()
x_test <-
df_test %>%
select(-was_promoted) %>%
as.matrix()
rm(df_test)
rm(model)
model <- keras_model_sequential()
model %>%
layer_dense(units = 150, activation = 'sigmoid', input_shape = c(dim(x_train)[2]))  %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 50, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 25, activation = 'relu') %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 2, activation = 'softmax')
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_sgd(),
metrics = c('accuracy')
)
history <- model %>% fit(
x_train, y_train,
epochs = 1, batch_size = 5000,
validation_split = 0.2
)
load('C:/Users/Samuel/Documents/ML_Naspers/Data/dataset_kerasready.RData')
sample_size <- 0.1
df <-
df %>%
mutate(rnd = runif(nrow(df))) %>%
filter(rnd < sample_size) %>%
select(-rnd)
rnd = runif(nrow(df))
df_test <- df[rnd > 0.8, ]
df_train <- df[rnd <= 0.8, ]
y_train <-
df_train %>%
select(was_promoted) %>%
as.matrix() %>%
to_categorical()
x_train <-
df_train %>%
select(-was_promoted) %>%
as.matrix()
y_test <-
df_test %>%
select(was_promoted) %>%
as.matrix() %>%
to_categorical()
x_test <-
df_test %>%
select(-was_promoted) %>%
as.matrix()
rm(model)
model <- keras_model_sequential()
model %>%
layer_dense(units = 150, activation = 'sigmoid', input_shape = c(dim(x_train)[2]))  %>%
layer_dropout(rate = 0.4) %>%
layer_dense(units = 50, activation = 'relu') %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 25, activation = 'relu') %>%
layer_dropout(rate = 0.2) %>%
layer_dense(units = 2, activation = 'softmax')
losses <- c('categorical_crossentropy', 'binary_crossentropy', 'categorical_hinge')
metrics <- c('accuracy', 'binary_accuracy','categorical_accuracy','sparse_categorical_accuracy')
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_sgd(),
metrics = c('accuracy')
)
history <- model %>% fit(
x_train, y_train,
epochs = 1, batch_size = 5000,
validation_split = 0.2
)
library(keras)
library(dplyr)
# load the data + sample
load('C:/Users/Samuel/Documents/ML_Naspers/Data/dataset_kerasready.RData')
sample_size <- 0.1
df <-
df %>%
mutate(rnd = runif(nrow(df))) %>%
filter(rnd < sample_size) %>%
select(-rnd)
rnd = runif(nrow(df))
# partition
df_test <- df[rnd > 0.8, ]
df_train <- df[rnd <= 0.8, ]
rm(df, rnd, sample_size)
# Neural
y_train <-
df_train %>%
select(was_promoted) %>%
unname() %>%
as.matrix() %>%
to_categorical()
x_train <-
df_train %>%
select(-was_promoted) %>%
unname() %>%
as.matrix()
y_test <-
df_test %>%
select(was_promoted) %>%
unname() %>%
as.matrix() %>%
to_categorical()
x_test <-
df_test %>%
select(-was_promoted) %>%
unname() %>%
as.matrix()
rm(model, history)
model <- keras_model_sequential()
model %>%
layer_dense(units = 128, input_shape = c(dim(x_train)[2]))  %>%
layer_activation_leaky_relu(alpha = 0.3) %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 64, activation = 'relu', input_shape = c(dim(x_train)[2]))  %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 2, activation = 'softmax')
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_adam(lr=0.008),
metrics = c('accuracy')
)
history <- model %>% fit(
x_train, y_train,
epochs = 10, batch_size = 128,
validation_split = 0.3
)
##### ANN #####
rm(list=ls())
install_keras(tensorflow = 'gpu')
library(keras)
install_keras(tensorflow = 'gpu')
library(keras)
library(dplyr)
library(mlr)
load('C:/Users/Samuel/Documents/ML_Naspers/Data/dataset_kerasready.RData')
str(df)
sample_size <- 0.1
df <-
df %>%
mutate(rnd = runif(nrow(df))) %>%
filter(rnd < sample_size) %>%
select(-rnd)
rnd = runif(nrow(df))
# partition
df_test <- df[rnd > 0.8, ]
df_train <- df[rnd <= 0.8, ]
str(df_train)
rm(df, rnd, sample_size)
y_train <-
df_train %>%
select(was_promoted) %>%
unname() %>%
as.matrix() %>%
to_categorical()
x_train <-
df_train %>%
select(-was_promoted) %>%
unname() %>%
as.matrix()
rm(df_train)
y_test <-
df_test %>%
select(was_promoted) %>%
unname() %>%
as.matrix() %>%
to_categorical()
x_test <-
df_test %>%
select(-was_promoted) %>%
unname() %>%
as.matrix()
rm(df_test)
rm(model, history)
model <- keras_model_sequential()
model %>%
layer_dense(units = 128, input_shape = c(dim(x_train)[2]))  %>%
layer_activation_leaky_relu(alpha = 0.3) %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 64, activation = 'relu', input_shape = c(dim(x_train)[2]))  %>%
layer_dropout(rate = 0.3) %>%
layer_dense(units = 2, activation = 'softmax')
model %>% compile(
loss = 'categorical_crossentropy',
optimizer = optimizer_adam(lr=0.008),
metrics = c('accuracy')
)
history <- model %>% fit(
x_train, y_train,
epochs = 10, batch_size = 128,
validation_split = 0.3
)
